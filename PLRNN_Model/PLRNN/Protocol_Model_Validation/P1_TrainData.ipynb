{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7e6bba",
   "metadata": {},
   "source": [
    "# Protocol Validation Hyperparameters 1: Training Data\n",
    "\n",
    "The valuation of the hyperparameters are based on the next points:\n",
    "\n",
    "1) Correlation between Train model trial and Data per neuron\n",
    "2) Mean Square Error between model trial and Date per neuron\n",
    "3) Power Spectrum Error\n",
    "4) Kullback Leibler Divergence\n",
    "5) Checking the A parameters to know the long-term behaviour of the model\n",
    "\n",
    "This script compute the previous measurents from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Import Libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import matplotlib.patches as mpatches\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bptt.models import Model\n",
    "from evaluation import klx_gmm as kl\n",
    "from evaluation import mse as ms\n",
    "from evaluation import pse as ps\n",
    "from function_modules import model_anafunctions as func\n",
    "\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0047d03",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "Loading functions needed for the main script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c66ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%% FUNCTIONS\n",
    "\n",
    "def openhyper(mpath):\n",
    "    file=open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),'rb')\n",
    "    hyper=pickle.load(file)\n",
    "    file.close()\n",
    "    return hyper\n",
    "\n",
    "def Hyper_mod(mpath,data_path):\n",
    "    file=open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),'rb')\n",
    "    hyper=pickle.load(file)\n",
    "    file.close()\n",
    "    hyper['data_path']=os.path.join(data_path,'Training_data.npy').replace('\\\\','/')\n",
    "    hyper['inputs_path']=os.path.join(data_path,'Training_inputs.npy').replace('\\\\','/')\n",
    "    full_name = open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),\"wb\")                      # Name for training data\n",
    "    pickle.dump(hyper,full_name)            # Save train data\n",
    "    #close save instance \n",
    "    full_name.close()\n",
    "\n",
    "def Training_eval(m_pathway,run,data_path,NeuronPattern):\n",
    "    mpath=os.path.join(m_pathway,run).replace('\\\\','/')\n",
    "    Hyper_mod(mpath,data_path)\n",
    "    #### Load model\n",
    "    hyper = openhyper(mpath)\n",
    "    save_files=os.listdir(mpath)\n",
    "    save_models=[s for s in save_files if \"model\" in s]\n",
    "    num_epochs = len(save_models)*hyper[\"save_step\"]\n",
    "    m = Model()\n",
    "    m.init_from_model_path(mpath, epoch=num_epochs)\n",
    "\n",
    "    At, _, _, _, _, _ = m.get_latent_parameters()\n",
    "    # Transform tensor to numpy format\n",
    "    A = At.detach().numpy()\n",
    "\n",
    "    # General Parameters\n",
    "    num_trials=len(NeuronPattern[\"Training_Neuron\"])\n",
    "    num_neurons=NeuronPattern[\"Training_Neuron\"][0].shape[1]\n",
    "\n",
    "    # Generate Latent states for Train Trials\n",
    "    ModelS=[]\n",
    "    for w_index in range(num_trials):\n",
    "        data_trial=tc.from_numpy(NeuronPattern[\"Training_Neuron\"][w_index]).float()          # tensor of neuronal data for initial trial data\n",
    "        input_trial = tc.from_numpy(NeuronPattern[\"Training_Input\"][w_index]).float()\n",
    "        length_sim = input_trial.shape[0]\n",
    "        X, _ = m.generate_free_trajectory(data_trial,input_trial,length_sim,w_index)\n",
    "        ModelS.append(X[:,:])\n",
    "\n",
    "    # Correlation between Train model trial and Data per neuron\n",
    "    DT=[tc.from_numpy(NeuronPattern[\"Training_Neuron\"][i]).float() for i in range(len(NeuronPattern[\"Training_Neuron\"]))]\n",
    "    N = ModelS[1].size(1)                                                                          # number of neurons\n",
    "    NT = len(ModelS)\n",
    "    rs = tc.zeros((N,NT))                                                                       # initialization of the correlation variable\n",
    "\n",
    "    for nt in range(NT):\n",
    "        eps = tc.randn_like(ModelS[nt]) * 1e-5                                                          # generation of little noise to avoid problems with silent neurons\n",
    "        X_eps_noise = ModelS[nt] + eps                                                                  # adding noise to the signal \n",
    "        for n in range(N):\n",
    "            rs[n,nt] = func.pearson_r(X_eps_noise[:, n], DT[nt][:, n])                                      # computation of the pearson correlation\n",
    "    rs = rs.detach().numpy()\n",
    "\n",
    "    MEAN_Corre=rs.mean()\n",
    "\n",
    "    # Mean Square Error between model trial and Date per neuron\n",
    "    n_steps=100\n",
    "    val_mse = np.empty((n_steps,num_trials))\n",
    "    for indices in range(num_trials):\n",
    "        input= tc.from_numpy(NeuronPattern[\"Training_Input\"][indices]).float()\n",
    "        data = tc.from_numpy(NeuronPattern[\"Training_Neuron\"][indices]).float()\n",
    "        val_mse[:,indices] = ms.n_steps_ahead_pred_mse(m, data, input, n_steps, indices)\n",
    "    MEAN_mse = np.mean(val_mse)\n",
    "\n",
    "    # Kullback Leibler Divergence\n",
    "\n",
    "    Model_Signal,_= func.concatenate_list(ModelS,0)\n",
    "    Data_Signal,_= func.concatenate_list(NeuronPattern[\"Training_Neuron\"],0)\n",
    "\n",
    "    Dim_kl = int(np.floor(num_neurons/3))\n",
    "    neu_list = np.array([1,2,3])\n",
    "    kl_dim = np.ones([Dim_kl,1])*np.nan\n",
    "    for j in range(Dim_kl):\n",
    "        kl_dim[j] = kl.calc_kl_from_data(tc.tensor(Model_Signal[:,neu_list]),\n",
    "                                            tc.tensor(Data_Signal[:,neu_list]))\n",
    "        neu_list += 3\n",
    "\n",
    "    MEAN_kl = kl_dim.mean()\n",
    "\n",
    "    # Power Spectrum Error\n",
    "    MEAN_pse,pse_list = ps.power_spectrum_error(tc.tensor(Model_Signal), tc.tensor(Data_Signal))\n",
    "\n",
    "    # Checking divergence of the model by A parameter\n",
    "    #True: The system will diverge at some point\n",
    "    #False: The system will never diverge\n",
    "    A_divergence=sum(A>1)>0\n",
    "\n",
    "    # FIGURES\n",
    "    plt.figure()\n",
    "    for it in range(NT):\n",
    "        plt.hist(rs[:,it],alpha=0.3)\n",
    "    plt.xlabel(\"Corr(Model vs Data)\")\n",
    "    plt.ylabel(\"neurons\")\n",
    "    plt.title(\"Distribution Train Trials\")\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Train_Distr_Corr.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    Higher=np.array([np.where(rs[:,i]>0.4)[0].shape[0]/N for i in range(NT)])\n",
    "    Lower=np.array([np.where(rs[:,i]<0.4)[0].shape[0]/N for i in range(NT)])\n",
    "    Trials = [i for i in range(NT)]\n",
    "    Ratio_neurons = {\n",
    "        \">0.4\": Higher,\n",
    "        \"<0.4\": Lower,\n",
    "    }\n",
    "    width = 0.5\n",
    "    fig, ax = plt.subplots()\n",
    "    bottom = np.zeros(NT)\n",
    "    for boolean, weight_count in Ratio_neurons.items():\n",
    "        p = ax.bar(Trials, weight_count, width, label=boolean, bottom=bottom)\n",
    "        bottom += weight_count\n",
    "    ax.set_title(\"Neuron Groups Train Trials\")\n",
    "    ax.set_ylabel(\"Ratio Neurons\")\n",
    "    ax.set_xlabel(\"Trials\")\n",
    "    lgd=ax.legend(title=\"Correlation\",bbox_to_anchor=(1.1, 1.05))\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Train_Distr_Corr_Trial.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(val_mse.mean(0))\n",
    "    plt.xlabel(\"Trials\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Train_MSE.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(NT),rs.mean(0))\n",
    "    plt.xlabel(\"Trials\")\n",
    "    plt.ylabel(\"Mean Correlation\")\n",
    "    plt.ylim([0,1])\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Train_Correlation.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(Dim_kl),kl_dim)\n",
    "    plt.xlabel(\"Subsets of 3 neurons\")\n",
    "    plt.ylabel(\"KLx\")\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Train_KLx.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    neu=np.random.choice(num_neurons,3,replace=False)\n",
    "    ax = plt.figure(figsize=(10,10)).add_subplot(projection='3d')\n",
    "    ax.plot(Model_Signal[:,neu[0]], Model_Signal[:,neu[1]],\n",
    "            Model_Signal[:,neu[2]], 'red',linestyle='dashed',label=\"Generated\")\n",
    "    ax.plot(Data_Signal[:,neu[0]], Data_Signal[:,neu[1]], \n",
    "            Data_Signal[:,neu[2]], 'blue',linestyle='dashed',label=\"Real\")\n",
    "    lgd=ax.legend()\n",
    "    ax.set_xlabel('Neu 1',labelpad =15)\n",
    "    ax.set_ylabel('Neu 2',labelpad =15)\n",
    "    ax.set_zlabel('Neu 3',labelpad =15)\n",
    "    ax.set_title('Training')\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Train_PhasePlane.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(N),pse_list)\n",
    "    plt.xlabel(\"Neurons\")\n",
    "    plt.ylabel(\"PSE\")\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Train_PSE.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    #Hyper-Parameters of the Model\n",
    "    Model_Hyper={}\n",
    "    #Identification Hidden Units\n",
    "    Model_Hyper[\"HU\"] = hyper['dim_hidden']\n",
    "    #Identification Parameter Lambda 1\n",
    "    Model_Hyper[\"L1\"] = hyper['reg_lambda1'][0]\n",
    "    #Identification Parameter Lambda 2\n",
    "    Model_Hyper[\"L2\"] = hyper['reg_lambda2'][0]\n",
    "    #Identification Parameter Lambda 2\n",
    "    Model_Hyper[\"L3\"] = hyper['reg_lambda3'][0]\n",
    "    #Identification Sequence Length\n",
    "    Model_Hyper[\"SL\"] = hyper['seq_len']\n",
    "    #Identification A_regularization\n",
    "    Model_Hyper[\"AR\"]=hyper[\"A_reg\"]\n",
    "    \n",
    "    #Evaluation of the Model\n",
    "    Model_Eval={}\n",
    "    #Mean Correlation of the Session vs Model\n",
    "    Model_Eval[\"Correlation\"] = MEAN_Corre\n",
    "    #Mean MSE testing in 100 sections for each trial\n",
    "    Model_Eval[\"MSE\"] = MEAN_mse\n",
    "    #Mean PSE of the whole session. \n",
    "    Model_Eval[\"PSE\"] = MEAN_pse\n",
    "    #Mean Kullback leibler divergence of the whole session\n",
    "    Model_Eval[\"KLx\"] = MEAN_kl\n",
    "    #Checking the A parameters to know the long-term behaviour of the model\n",
    "    Model_Eval[\"Divergence\"] = A_divergence\n",
    "    \n",
    "    return Model_Hyper,Model_Eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff319f",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Data used in this script are the original neuronal activity and the model trained. In this script we test the models trained for a specific dataset to determine optimal hyperparameters.\n",
    "\n",
    "The input needed in this script are the directories of the original data used for training and the different models:\n",
    "\n",
    "1) data_path: pathway where the original data used for training the PLRNN is saved\n",
    "2) model_path: general directory where the different models (different hyperparameters) are saved\n",
    "3) save_path: the directory where you want to save the Dataframe generated by the script\n",
    "4) save_name: name of the file generated ('TrainEvaluation_'+save_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70633049",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# Set Paths #######################################################\n",
    "# Select Path for Data (Training and Test Trials)\n",
    "data_path = 'D:\\\\_work_cestarellas\\\\Analysis\\\\PLRNN\\\\noautoencoder\\\\neuralactivity\\\\OFC\\\\CE17\\\\L6\\\\Test0\\\\datasets' \n",
    "# Select Path for Models (Folder containing the specific models to test)\n",
    "model_path = 'D:\\\\_work_cestarellas\\\\Analysis\\\\PLRNN\\\\noautoencoder\\\\results\\\\Tuning_OFC_CE17_221008'\n",
    "# Select Path for saving Data:\n",
    "save_path = 'D:\\\\_work_cestarellas\\\\Analysis\\\\PLRNN\\\\noautoencoder\\\\results\\\\Tuning_OFC_CE17_221008\\\\Evaluation_Sheets'\n",
    "# Select the name for the save file (session name):\n",
    "save_name='CE17_221008'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Load data ##########################################################\n",
    "\n",
    "# Load Training & Test Data\n",
    "train_n,train_i = func.load_data(data_path,'Training')\n",
    "Data_info={\"Training_Neuron\":train_n,\"Training_Input\":train_i}\n",
    "\n",
    "######################################## Test measurements #######################################################\n",
    "\n",
    "# Computation of testing measurements for the models in your model_path\n",
    "model_list=next(os.walk(model_path))[1]\n",
    "#Initialization of evaluations lists\n",
    "Correlation=[]\n",
    "PSE=[]\n",
    "NMSE = []\n",
    "KLx=[]\n",
    "Div=[]\n",
    "#Initialization of hyperparameter lists\n",
    "Model_name=[]\n",
    "RunNumber=[]\n",
    "hidden=[]\n",
    "lm1=[]\n",
    "lm2=[]\n",
    "lm3=[]\n",
    "sl=[]\n",
    "a_reg=[]\n",
    "\n",
    "for i in tqdm(model_list,\"Testing Models: \"):\n",
    "    pathway=os.path.join(model_path,i).replace('\\\\','/')\n",
    "    runs=next(os.walk(pathway))[1] # taking only the folders with the models\n",
    "    for j in runs:\n",
    "        Hyper,Eval= Training_eval(pathway,j,data_path,Data_info)\n",
    "        # List of evaluations\n",
    "        NMSE.append(Eval[\"MSE\"])\n",
    "        Correlation.append(Eval[\"Correlation\"])\n",
    "        PSE.append(Eval[\"PSE\"])\n",
    "        KLx.append(Eval[\"KLx\"])\n",
    "        Div.append(Eval[\"Divergence\"])\n",
    "        # List of Hyper-parameters\n",
    "        # Folder's name of the model\n",
    "        Model_name.append(i)\n",
    "        # Number of the run\n",
    "        RunNumber.append(j)\n",
    "        #Identification Hidden Units\n",
    "        hidden.append(Hyper[\"HU\"])\n",
    "        #Identification Parameter Lambda 1\n",
    "        lm1.append(Hyper[\"L1\"])\n",
    "        #Identification Parameter Lambda 2\n",
    "        lm2.append(Hyper[\"L2\"])\n",
    "        #Identification Parameter Lambda 3\n",
    "        lm3.append(Hyper[\"L3\"])\n",
    "        #Identification Sequence Length\n",
    "        sl.append(Hyper[\"SL\"])\n",
    "        #Activation of regulation A matrix\n",
    "        a_reg.append(Hyper[\"AR\"])\n",
    "\n",
    "\n",
    "############################################### Saving ############################################################\n",
    "\n",
    "# Saving Data as DataFrame\n",
    "TrainData={\"Models\":Model_name,\"Runs\":RunNumber,\n",
    "           \"Hiddn_Units\":hidden,\"Sequence_Length\":sl,\"Regulation_A\":a_reg,\n",
    "           \"Lambda1\":lm1,\"Lambda2\":lm2,\"Lambda3\":lm3,\n",
    "           \"Correlation\":Correlation,\"NMSE\":NMSE,\"PSE\":PSE,\n",
    "           \"KLx\":KLx,\"Divergence\":Div\n",
    "          }\n",
    "Traindf=pd.DataFrame(TrainData)\n",
    "\n",
    "# Check/Create Path\n",
    "if os.path.exists(save_path):\n",
    "    os.chdir(save_path)\n",
    "else:\n",
    "    os.makedirs(save_path)\n",
    "    os.chdir(save_path)\n",
    "save_file='TrainEvaluation_'+save_name+'.csv'\n",
    "Traindf.to_csv(save_file,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
