{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e88c8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cestarellas\\anaconda3\\envs\\PLAna\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# %% Import Libraries\n",
    "#This is a test\n",
    "import os\n",
    "import pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm import tqdm\n",
    "from bptt.models import Model\n",
    "import model_anafunctions as func\n",
    "\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579b9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Path for Models (Folder containing the specific models to test)\n",
    "model_path = 'D:\\\\_work_cestarellas\\Analysis\\\\Pack_Daniel_project\\\\Preprocess_model\\\\bptt_JG15_25\\\\results'\n",
    "#model_path = 'D:/_work_cestarellas/Analysis/PLRNN/noautoencoder/results/Tuning_OFC_CE17_221008'\n",
    "\n",
    "# Loading models and simulations\n",
    "model_name = 'DataTrainingH768_lm1_1e-05_lm2_128_lm3_00_seql_400/001'\n",
    "#model_name = 'CE1701_HU_256_l1_0.001_l2_08_l3_00_SL_400_encdim_65/001'\n",
    "\n",
    "## Loading recorded neuronal activity (pre-processesed)\n",
    "mpath=os.path.join(model_path,model_name).replace('\\\\','/')\n",
    "\n",
    "file=open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),'rb')\n",
    "hyper=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cbeefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment': 'JG15_25',\n",
       " 'base_path': './results',\n",
       " 'name': 'DataTrainingH768_lm1_1e-05_lm2_128_lm3_00_seql_400',\n",
       " 'run': 1,\n",
       " 'loss_fn': 'MSE',\n",
       " 'use_gpu': 1,\n",
       " 'device_id': 0,\n",
       " 'no_printing': 1,\n",
       " 'use_tb': 1,\n",
       " 'metrics': ['mse', 'pse'],\n",
       " 'test': 0,\n",
       " 'data_path': 'D:/_work_cestarellas/Analysis/Pack_Daniel_project/Preprocess_model/bptt_JG15_25/neuralactivity/JG15_25_DP/datasets/Training_data.npy',\n",
       " 'inputs_path': 'D:/_work_cestarellas/Analysis/Pack_Daniel_project/Preprocess_model/bptt_JG15_25/neuralactivity/JG15_25_DP/datasets/Training_inputs.npy',\n",
       " 'load_model_path': None,\n",
       " 'resume_epoch': None,\n",
       " 'dim_z': 51,\n",
       " 'dim_hidden': 768,\n",
       " 'W_trial': 1,\n",
       " 'h_trial': 0,\n",
       " 'clip_range': 10,\n",
       " 'model': 'clipped-shPLRNN',\n",
       " 'latent_model': 'clipped-shPLRNN',\n",
       " 'n_bases': 5,\n",
       " 'mean_center': 0,\n",
       " 'learn_z0': 0,\n",
       " 'enc_dim': 51,\n",
       " 'AE_learning_rate': 0.001,\n",
       " 'AE_weight_decay': 1e-07,\n",
       " 'pretrain_epochs': 50000,\n",
       " 'observation_noise_level': 0.05,\n",
       " 'process_noise_level': 0.0,\n",
       " 'pm_noise': 0.0,\n",
       " 'reconstruction_loss_weight': 1.0,\n",
       " 'prediction_loss_weight': 1.0,\n",
       " 'latent_loss_weight': 1.0,\n",
       " 'autoencoder_type': 'Identity',\n",
       " 'n_layers': 1,\n",
       " 'activation_fn': 'Tanh',\n",
       " 'only_AE': 0,\n",
       " 'TF_alpha': 0.5,\n",
       " 'TF_alpha2': 0.001,\n",
       " 'TF_alpha_decay': 1.0,\n",
       " 'TF_alpha_estim': 0.0,\n",
       " 'batch_size': 256,\n",
       " 'batches_per_epoch': 1,\n",
       " 'seq_len': 400,\n",
       " 'save_step': 50000,\n",
       " 'save_img_step': 50000,\n",
       " 'annealing': 2,\n",
       " 'var_sql': 0,\n",
       " 'learning_rate': 0.001,\n",
       " 'n_epochs': 200000,\n",
       " 'gradient_clipping': 10.0,\n",
       " 'use_reg': 1,\n",
       " 'reg_type': 2,\n",
       " 'reg_ratios': [1.0],\n",
       " 'reg_ratios_W': [1.0],\n",
       " 'reg_lambda1': [1e-05],\n",
       " 'reg_lambda2': [128.0],\n",
       " 'reg_lambda3': [0.0],\n",
       " 'reg_norm': 'l2',\n",
       " 'A_reg': 0,\n",
       " 'C_reg': 0,\n",
       " 'use_pruning': 0,\n",
       " 'pruning_epoch': 1000,\n",
       " 'gtf_alpha_decay': 0.997,\n",
       " 'gtf_alpha': 0.5,\n",
       " 'aGTF_cycle_length': 5,\n",
       " 'aGTF_method': 'arithmetic_mean',\n",
       " 'n_trials': 35,\n",
       " 'dim_x': 51,\n",
       " 'dim_s': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a08571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% FUNCTIONS\n",
    "def Hyper_mod(mpath,data_path):\n",
    "    file=open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),'rb')\n",
    "    hyper=pickle.load(file)\n",
    "    file.close()\n",
    "    hyper['data_path']=os.path.join(data_path,'Training_data.npy').replace('\\\\','/')\n",
    "    hyper['inputs_path']=os.path.join(data_path,'Training_inputs.npy').replace('\\\\','/')\n",
    "    print(hyper['device_id'])\n",
    "    hyper['device_id'] = 0\n",
    "    print(hyper['device_id'])    \n",
    "    full_name = open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),\"wb\")                      # Name for training data\n",
    "    pickle.dump(hyper,full_name)            # Save train data\n",
    "    #close save instance \n",
    "    full_name.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6e8bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Chosen model is clipped shPLRNN, the bases Parameter has no effect here!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on CUDA device 3 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150000\u001b[39m\n\u001b[0;32m     24\u001b[0m m \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m---> 25\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_from_model_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Generation Training Data\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\PLRNN\\Paper_Figures\\Figure_3\\bptt\\models.py:145\u001b[0m, in \u001b[0;36mModel.init_from_model_path\u001b[1;34m(self, model_path, epoch)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_submodules()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# restore model parameters\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_statedict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\PLRNN\\Paper_Figures\\Figure_3\\bptt\\models.py:231\u001b[0m, in \u001b[0;36mModel.load_statedict\u001b[1;34m(self, model_path, model_name, epoch)\u001b[0m\n\u001b[0;32m    229\u001b[0m     epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    230\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name, \u001b[38;5;28mstr\u001b[39m(epoch)))\n\u001b[1;32m--> 231\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state_dict\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PLAna\\lib\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PLAna\\lib\\site-packages\\torch\\serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PLAna\\lib\\site-packages\\torch\\serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1015\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1016\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PLAna\\lib\\site-packages\\torch\\serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[1;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PLAna\\lib\\site-packages\\torch\\serialization.py:176\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 176\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PLAna\\lib\\site-packages\\torch\\serialization.py:152\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 152\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    154\u001b[0m             storage_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PLAna\\lib\\site-packages\\torch\\serialization.py:143\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    141\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on CUDA device \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    144\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but torch.cuda.device_count() is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    145\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load with map_location to map your storages \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    146\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto an existing device.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on CUDA device 3 but torch.cuda.device_count() is 1. Please use torch.load with map_location to map your storages to an existing device."
     ]
    }
   ],
   "source": [
    "#%% Load Data and Behaviour\n",
    "################################ Directories and files to modify ################################ \n",
    "# Select Path for multi-unit data\n",
    "data_path = 'D:\\\\_work_cestarellas\\\\Analysis\\\\Pack_Daniel_project\\\\Preprocess_model\\\\bptt_JG15_25\\\\neuralactivity\\\\JG15_25_DP\\\\datasets\\\\' \n",
    "#data_path = 'D:/_work_cestarellas/Analysis/PLRNN/noautoencoder/neuralactivity/OFC/CE17/L6/Test0/datasets/' \n",
    "# Select Path for Models (Folder containing the specific models to test)\n",
    "model_path = 'D:\\\\_work_cestarellas\\Analysis\\\\Pack_Daniel_project\\\\Preprocess_model\\\\bptt_JG15_25\\\\results'\n",
    "#model_path = 'D:/_work_cestarellas/Analysis/PLRNN/noautoencoder/results/Tuning_OFC_CE17_221008'\n",
    "\n",
    "# Loading models and simulations\n",
    "model_name = 'DataTrainingH768_lm1_1e-05_lm2_128_lm3_00_seql_400/001'\n",
    "#model_name = 'CE1701_HU_256_l1_0.001_l2_08_l3_00_SL_400_encdim_65/001'\n",
    "\n",
    "## Loading recorded neuronal activity (pre-processesed)\n",
    "mpath=os.path.join(model_path,model_name).replace('\\\\','/')\n",
    "\n",
    "train_n,train_i = func.load_data(data_path,'Training')\n",
    "test_n,test_i = func.load_data(data_path,'Test')\n",
    "\n",
    "Hyper_mod(mpath,data_path)\n",
    "\n",
    "# Loading Model\n",
    "num_epochs = 150000\n",
    "m = Model()\n",
    "m.init_from_model_path(mpath, epoch=num_epochs)\n",
    "m.eval()\n",
    "\n",
    "# Generation Training Data\n",
    "ModelS=[]\n",
    "for w_index in tqdm(range(len(train_n))):\n",
    "    data_trial=tc.from_numpy(train_n[w_index]).float()          # tensor of neuronal data for initial trial data\n",
    "    input_trial = tc.from_numpy(train_i[w_index]).float()\n",
    "    length_sim = input_trial.shape[0]\n",
    "    X, _ = m.generate_free_trajectory(data_trial,input_trial,length_sim,w_index)\n",
    "    ModelS.append(X[:,:])\n",
    "\n",
    "# Concatenating signals\n",
    "Nseries,_=func.concatenate_list(train_n,0)          # recorded activity\n",
    "Iseries,_=func.concatenate_list(train_i,0)          # external input\n",
    "Mseries,_=func.concatenate_list(ModelS,0)           # simulated activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82a3ac09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Classifier 1: Choice (Gamble or Safe)\n",
    "# Identification of training trials in full behaviour (Tr_behaviour)\n",
    "Meta_file_path = \"D:\\\\_work_cestarellas\\\\Analysis\\\\Pack_Daniel_project\\\\Preprocess_model\\\\bptt_JG15_24\\\\neuralactivity\\\\JG15_24_DP\\\\datasets\\\\Metadata.pkl\"\n",
    "# Open the file in binary mode\n",
    "with open(Meta_file_path, \"rb\") as f:\n",
    "    # Load the pickled data\n",
    "    Meta_info = pickle.load(f)\n",
    "\n",
    "Test_trials = Meta_info[\"TestTrials\"]; Training_trials = Meta_info[\"TrainingTrials\"]    \n",
    "    \n",
    "itrain = 0\n",
    "itest = 0\n",
    "Trials_inmodel = []\n",
    "for i in range(len(Meta_info[\"TestTrials\"])+len(Meta_info[\"TrainingTrials\"])):\n",
    "    if i in Meta_info[\"TrainingTrials\"]:\n",
    "        Trials_inmodel.append(np.sum(np.diff(train_i[itrain][:,0])==1))\n",
    "        itrain += 1\n",
    "    elif i in Meta_info[\"TestTrials\"]:\n",
    "        Trials_inmodel.append(np.sum(np.diff(test_i[itest][:,0])==1))\n",
    "        itest +=1\n",
    "        \n",
    "# Cumulative sum of the behavioural trials in concatenated trials        \n",
    "CUM_trials = [np.sum(Trials_inmodel[:i]) for i in range(len(Trials_inmodel)+1)]\n",
    "\n",
    "Ini_beh = [0]+[CUM_trials[i+1] for i in Test_trials]\n",
    "End_beh = [CUM_trials[i] for i in Test_trials]+[CUM_trials[-1]]\n",
    "\n",
    "Tr_behaviour = []\n",
    "for k in range(len(Ini_beh)):\n",
    "    Tr_behaviour= Tr_behaviour+[i for i in range(Ini_beh[k],End_beh[k])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Behaviour. Obtaining information to be classified.\n",
    "# Info Columns BehData:\n",
    "# 0- Trial Start\n",
    "# 1- Trial End\n",
    "# 2- Duration (Seconds)\n",
    "# 3- Block\n",
    "# 4- Gamble Arm (Right = 1, Left = 0)\n",
    "# 5- Probability big Reward\n",
    "# 6- Probability Small Reward\n",
    "# 7- Ammount Big Reward\n",
    "# 8- Ammount Small Reward\n",
    "# 9- Number of previously wheel not stopping\n",
    "# 10- Not responding Trial\n",
    "# 11- Chosen Side (Right = 1, Left = 0)\n",
    "# 12- Chosen Arm (Gamble = 1, Safe = 0)\n",
    "# 13- Reward Given\n",
    "# 14- Start of the trial (Sampling points)\n",
    "# 15- Cue Presentation (Sampling Points)\n",
    "# 16- Start of the response window (Sampling Points)\n",
    "# 17- Reward Period (Sampling Points)\n",
    "# 18- End of the trial\n",
    "\n",
    "path='D:/_work_cestarellas/Analysis/PLRNN/Session_Selected/OFC/JG15_190724_clustered'   # Pathway of the data (behaviour & Spike activity)\n",
    "\n",
    "# Selection of the file\n",
    "os.chdir(path)\n",
    "list_files = os.listdir(path)\n",
    "for i in list_files:\n",
    "    if i.find('Behaviour')>0:\n",
    "        Behaviour_name = i\n",
    "\n",
    "# Load data\n",
    "# Open the Behaviour file\n",
    "Bdata = scipy.io.loadmat(Behaviour_name)\n",
    "BehData = Bdata[list(Bdata.keys())[-1]]\n",
    "\n",
    "# Classification of trials following the behabiour\n",
    "GambleRewardTrials = np.where((BehData[:,12]==1) & (BehData[:,13]==1))[0]\n",
    "GambleNoRewardTrials =  np.where((BehData[:,12]==1) & (BehData[:,13]==0))[0]\n",
    "SafeRewardTrials = np.where((BehData[:,12]==0) & (BehData[:,13]==1))[0]\n",
    "SafeNoRewardTrials = np.where((BehData[:,12]==0) & (BehData[:,13]==0))[0]\n",
    "NoRespondingTrials = np.where(BehData[:,10]==1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of the Decision in the trials with previous information\n",
    "# The previous information is the mean firing rate of each neuron from wheel stop up to reward\n",
    "\n",
    "# Next Decision (G,S)\n",
    "ND_se = (BehData[:,12]==1)*1+(BehData[:,12]==0)*0\n",
    "ND = ND_se[Tr_behaviour]\n",
    "\n",
    "# Firing Rate Neurons\n",
    "\n",
    "# Temporal sectors\n",
    "CueTime_end = np.where(np.diff(Iseries[:,0])==-1)[0]\n",
    "StopTime = np.where(np.diff(Iseries[:,0])==1)[0]-49\n",
    "# Data Set Mean Activity Zscore Neurons\n",
    "num_trials = len(CueTime_end)\n",
    "num_neurons = Nseries.shape[1]\n",
    "X_data = np.zeros((num_trials,num_neurons))\n",
    "X_model = np.zeros((num_trials,num_neurons))\n",
    "for it in range(num_trials):\n",
    "    X_data[it,:]=np.mean(Nseries[StopTime[it]:CueTime_end[it],:],0)\n",
    "    X_model[it,:]=np.mean(Mseries[StopTime[it]:CueTime_end[it],:],0)\n",
    "    \n",
    "\n",
    "#%% Next Decisions\n",
    "score_data=[]\n",
    "score_model=[]\n",
    "score_modelSh=[]\n",
    "score_dataSh=[]\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "for i in range(1000):\n",
    "    list_trials=np.linspace(0,X_data.shape[0]-1,X_data.shape[0]).astype(int)\n",
    "    np.random.shuffle(list_trials)\n",
    "    # Classifying data\n",
    "    X=X_data[list_trials,:]\n",
    "    y=ND[list_trials]\n",
    "    random_state=0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    clf.fit(X_train,y_train)\n",
    "    score_data.append(clf.score(X_test,y_test))\n",
    "\n",
    "    # Classifying model\n",
    "    Xm=X_model[list_trials,:]\n",
    "    random_state=0\n",
    "    X_trainM, X_testM, y_trainM, y_testM = train_test_split(\n",
    "        Xm, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    random_state=0\n",
    "    clf.fit(X_trainM,y_trainM)\n",
    "    score_model.append(clf.score(X_testM,y_testM))\n",
    "    \n",
    "    # Classifying data Shuffle\n",
    "    list_trials=np.linspace(0,X_data.shape[0]-1,X_data.shape[0]).astype(int)\n",
    "    y=ND[list_trials]\n",
    "    random_state=0\n",
    "    X_trainS, X_testS, y_trainS, y_testS = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "\n",
    "    clf.fit(X_trainS,y_trainS)\n",
    "    score_dataSh.append(clf.score(X_testS,y_testS))\n",
    "    # Classifying model Shuffle\n",
    "    X_trainMS, X_testMS, y_trainMS, y_testMS = train_test_split(\n",
    "        Xm, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    clf.fit(X_trainMS,y_trainMS)\n",
    "    score_modelSh.append(clf.score(X_testMS,y_testMS))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "Models = ['Data', 'Shuffle','Model','Shuffle']\n",
    "ax.boxplot([score_data,score_dataSh,score_model,score_modelSh],labels=Models)\n",
    "ax.set_ylabel('Test Score')\n",
    "ax.set_title('Choice (Gamble-Safe)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of the Type of Reward in the trials with information during reward and ITI period\n",
    "# The previous information is the mean firing rate of each neuron from Reward until the initial point of the following trial\n",
    "\n",
    "# Reward (Gamble, Safe, Nothing)\n",
    "RW_se = (BehData[:,12]==1)*(BehData[:,13])*2+(BehData[:,13])*(BehData[:,12]==0)*1\n",
    "RW = RW_se[Tr_behaviour]\n",
    "\n",
    "# Firing Rate Neurons\n",
    "\n",
    "# Temporal sectors\n",
    "RewTime_ini = np.where(np.diff(Iseries[:,0])==-1)[0]\n",
    "RT_end = np.where(np.diff(Iseries[:,0])==1)[0]-49\n",
    "RewTime_end = np.append(RT_end[1:],Iseries.shape[0])\n",
    "\n",
    "# Data Set Mean Activity Zscore Neurons\n",
    "num_trials = len(RewTime_end)\n",
    "num_neurons = Nseries.shape[1]\n",
    "X_data = np.zeros((num_trials,num_neurons))\n",
    "X_model = np.zeros((num_trials,num_neurons))\n",
    "for it in range(num_trials):\n",
    "    X_data[it,:]=np.mean(Nseries[RewTime_ini[it]:RewTime_end[it],:],0)\n",
    "    X_model[it,:]=np.mean(Mseries[RewTime_ini[it]:RewTime_end[it],:],0)\n",
    "    \n",
    "\n",
    "#%% Reward Identification\n",
    "score_data=[]\n",
    "score_model=[]\n",
    "score_modelSh=[]\n",
    "score_dataSh=[]\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "for i in range(1000):\n",
    "    list_trials=np.linspace(0,X_data.shape[0]-1,X_data.shape[0]).astype(int)\n",
    "    np.random.shuffle(list_trials)\n",
    "    # Classifying data\n",
    "    X=X_data[list_trials,:]\n",
    "    y=RW[list_trials]\n",
    "    random_state=0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    clf.fit(X_train,y_train)\n",
    "    score_data.append(clf.score(X_test,y_test))\n",
    "\n",
    "    # Classifying model\n",
    "    Xm=X_model[list_trials,:]\n",
    "    random_state=0\n",
    "    X_trainM, X_testM, y_trainM, y_testM = train_test_split(\n",
    "        Xm, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    random_state=0\n",
    "    clf.fit(X_trainM,y_trainM)\n",
    "    score_model.append(clf.score(X_testM,y_testM))\n",
    "    \n",
    "    # Classifying data Shuffle\n",
    "    list_trials=np.linspace(0,X_data.shape[0]-1,X_data.shape[0]).astype(int)\n",
    "    y=RW[list_trials]\n",
    "    random_state=0\n",
    "    X_trainS, X_testS, y_trainS, y_testS = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "\n",
    "    clf.fit(X_trainS,y_trainS)\n",
    "    score_dataSh.append(clf.score(X_testS,y_testS))\n",
    "    # Classifying model Shuffle\n",
    "    X_trainMS, X_testMS, y_trainMS, y_testMS = train_test_split(\n",
    "        Xm, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    clf.fit(X_trainMS,y_trainMS)\n",
    "    score_modelSh.append(clf.score(X_testMS,y_testMS))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "Models = ['Data', 'Shuffle','Model','Shuffle']\n",
    "ax.boxplot([score_data,score_dataSh,score_model,score_modelSh],labels=Models)\n",
    "ax.set_ylabel('Test Score')\n",
    "ax.set_title('Reward (Gamble-Safe-None)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of the Type of Reward in the trials with information during reward and ITI period\n",
    "# The previous information is the mean firing rate of each neuron from Reward until the initial point of the following trial\n",
    "\n",
    "# Blocks (High, Medium, Low)\n",
    "blocks=np.unique(BehData[:,5])\n",
    "if len(blocks)==3:\n",
    "    B_se=(BehData[:,5]==blocks[0])*0+(BehData[:,5]==blocks[1])*1+(BehData[:,5]==blocks[2])*2\n",
    "elif len(blocks)==2:\n",
    "    B_se=(BehData[:,5]==blocks[0])*0+(BehData[:,5]==blocks[1])*1\n",
    "elif len(blocks)<2:\n",
    "    print(\"There is no different blocks to classify the data\")\n",
    "else:\n",
    "    print(\"This is not the original task\")\n",
    "BI = B_se[Tr_behaviour]\n",
    "\n",
    "# Firing Rate Neurons\n",
    "\n",
    "# Temporal sectors\n",
    "TrialTime_ini = np.where(np.diff(Iseries[:,0])==1)[0]-49\n",
    "TrialTime_end = np.append(RT_end[1:],Iseries.shape[0])\n",
    "\n",
    "# Data Set Mean Activity Zscore Neurons\n",
    "num_trials = len(TrialTime_end)\n",
    "num_neurons = Nseries.shape[1]\n",
    "X_data = np.zeros((num_trials,num_neurons))\n",
    "X_model = np.zeros((num_trials,num_neurons))\n",
    "for it in range(num_trials):\n",
    "    X_data[it,:]=np.mean(Nseries[TrialTime_ini[it]:TrialTime_end[it],:],0)\n",
    "    X_model[it,:]=np.mean(Mseries[TrialTime_ini[it]:TrialTime_end[it],:],0)\n",
    "    \n",
    "\n",
    "#%% Reward Identification\n",
    "score_data=[]\n",
    "score_model=[]\n",
    "score_modelSh=[]\n",
    "score_dataSh=[]\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "for i in range(1000):\n",
    "    list_trials=np.linspace(0,X_data.shape[0]-1,X_data.shape[0]).astype(int)\n",
    "    np.random.shuffle(list_trials)\n",
    "    # Classifying data\n",
    "    X=X_data[list_trials,:]\n",
    "    y=BI[list_trials]\n",
    "    random_state=0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    clf.fit(X_train,y_train)\n",
    "    score_data.append(clf.score(X_test,y_test))\n",
    "\n",
    "    # Classifying model\n",
    "    Xm=X_model[list_trials,:]\n",
    "    random_state=0\n",
    "    X_trainM, X_testM, y_trainM, y_testM = train_test_split(\n",
    "        Xm, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    random_state=0\n",
    "    clf.fit(X_trainM,y_trainM)\n",
    "    score_model.append(clf.score(X_testM,y_testM))\n",
    "    \n",
    "    # Classifying data Shuffle\n",
    "    list_trials=np.linspace(0,X_data.shape[0]-1,X_data.shape[0]).astype(int)\n",
    "    y=BI[list_trials]\n",
    "    random_state=0\n",
    "    X_trainS, X_testS, y_trainS, y_testS = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "\n",
    "    clf.fit(X_trainS,y_trainS)\n",
    "    score_dataSh.append(clf.score(X_testS,y_testS))\n",
    "    # Classifying model Shuffle\n",
    "    X_trainMS, X_testMS, y_trainMS, y_testMS = train_test_split(\n",
    "        Xm, y, test_size=0.2, stratify=y, random_state=0\n",
    "        )\n",
    "    clf.fit(X_trainMS,y_trainMS)\n",
    "    score_modelSh.append(clf.score(X_testMS,y_testMS))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "Models = ['Data', 'Shuffle','Model','Shuffle']\n",
    "ax.boxplot([score_data,score_dataSh,score_model,score_modelSh],labels=Models)\n",
    "ax.set_ylabel('Test Score')\n",
    "ax.set_title('Blocks (High-Medium-Low)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a2a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1088c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
