{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7e6bba",
   "metadata": {},
   "source": [
    "# Protocol Validation Hyperparameters 2: Test Data\n",
    "\n",
    "The valuation of the hyperparameters are based on the next points:\n",
    "\n",
    "1) Mean Correlation of the Session vs Model\n",
    "2) Mean MSE testing in 100 sections for each trial\n",
    "3) Mean PSE of the whole session. \n",
    "4) Mean Kullback leibler divergence of the whole session\n",
    "5) Validation of test correlation \n",
    "\n",
    "This script compute the previous measurents from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Import Libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from bptt.models import Model\n",
    "from evaluation import klx_gmm as kl\n",
    "from evaluation import pse as ps\n",
    "from evaluation import mse as ms\n",
    "from function_modules import model_anafunctions as func\n",
    "\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0047d03",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "Loading functions needed for the main script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c66ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openhyper(mpath):\n",
    "    file=open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),'rb')\n",
    "    hyper=pickle.load(file)\n",
    "    file.close()\n",
    "    return hyper\n",
    "\n",
    "def Hyper_mod(mpath,data_path):\n",
    "    file=open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),'rb')\n",
    "    hyper=pickle.load(file)\n",
    "    file.close()\n",
    "    hyper['data_path']=os.path.join(data_path,'Training_data.npy').replace('\\\\','/')\n",
    "    hyper['inputs_path']=os.path.join(data_path,'Training_inputs.npy').replace('\\\\','/')\n",
    "    full_name = open(os.path.join(mpath,'hypers.pkl').replace(\"\\\\\",\"/\"),\"wb\")                      # Name for training data\n",
    "    pickle.dump(hyper,full_name)            # Save train data\n",
    "    #close save instance \n",
    "    full_name.close()\n",
    "\n",
    "def Testing_eval(m_pathway,run,data_path,NeuronPattern,Metadata):\n",
    "    mpath=os.path.join(m_pathway,run).replace('\\\\','/')\n",
    "    Hyper_mod(mpath,data_path)\n",
    "    #### Load model\n",
    "    hyper = openhyper(mpath)\n",
    "    save_files=os.listdir(mpath)\n",
    "    save_models=[s for s in save_files if \"model\" in s]\n",
    "    num_epochs = len(save_models)*hyper[\"save_step\"]\n",
    "    m = Model()\n",
    "    m.init_from_model_path(mpath, epoch=num_epochs)\n",
    "\n",
    "    _, W1t, W2t, _, _, Ct = m.get_latent_parameters()\n",
    "\n",
    "    # Transform tensor to numpy format\n",
    "    W2 = W2t.detach().numpy().transpose(1,2,0)\n",
    "    W1 = W1t.detach().numpy().transpose(1,2,0)\n",
    "    C = Ct.detach().numpy()\n",
    "    # General Parameters\n",
    "    Ntraining_trials=len(NeuronPattern[\"Training_Neuron\"])\n",
    "    Ntest_trials = len(NeuronPattern[\"Testing_Neuron\"])\n",
    "    num_neurons=NeuronPattern[\"Training_Neuron\"][0].shape[1]\n",
    "\n",
    "    # Generate Latent states for Test Trials\n",
    "    # Identificating Test Trials in the training trial set\n",
    "    t_prev = [i for i in Metadata[\"Training2Test\"]]\n",
    "    t_post = [i+1 for i in Metadata[\"Training2Test\"]]\n",
    "\n",
    "    # Computing W matrices for test trials\n",
    "    W2_test = np.empty((W2.shape[0],W2.shape[1],len(Metadata[\"TestTrials\"])))\n",
    "    W1_test = np.empty((W1.shape[0],W1.shape[1],len(Metadata[\"TestTrials\"])))\n",
    "    for i in range(len(t_prev)):\n",
    "        W2_test[:,:,i] = (W2[:,:,t_prev[i]]+W2[:,:,t_post[i]])/2.0\n",
    "        W1_test[:,:,i] = (W1[:,:,t_prev[i]]+W1[:,:,t_post[i]])/2.0\n",
    "    #Generate Latent states\n",
    "    ModelT = []\n",
    "    #Generate Latent states\n",
    "    W1_ind = [tc.from_numpy(W1_test[:,:,i]).float() for i in range(len(t_prev))]\n",
    "    W2_ind = [tc.from_numpy(W2_test[:,:,i]).float() for i in range(len(t_prev))]\n",
    "    for i in range(len(W1_ind)):\n",
    "        data_test=tc.from_numpy(NeuronPattern[\"Testing_Neuron\"][i]).float()\n",
    "        input_test=tc.from_numpy(NeuronPattern[\"Testing_Input\"][i]).float()\n",
    "        T0=NeuronPattern[\"Testing_Neuron\"][i].shape[0]\n",
    "        X, _ = m.generate_test_trajectory(data_test[0:11,:],W2_ind[i],W1_ind[i],input_test, T0,i)\n",
    "        ModelT.append(X)\n",
    "    \n",
    "    # Correlation between Train model trial and Data per neuron\n",
    "    DT=[tc.from_numpy(NeuronPattern[\"Testing_Neuron\"][i]).float() for i in range(len(NeuronPattern[\"Testing_Neuron\"]))]\n",
    "    rs = tc.zeros((num_neurons,Ntest_trials))                                                                       # initialization of the correlation variable\n",
    "\n",
    "    for nt in range(Ntest_trials):\n",
    "        eps = tc.randn_like(ModelT[nt]) * 1e-5                                                          # generation of little noise to avoid problems with silent neurons\n",
    "        X_eps_noise = ModelT[nt] + eps                                                                  # adding noise to the signal \n",
    "        for n in range(num_neurons):\n",
    "            rs[n,nt] = func.pearson_r(X_eps_noise[:, n], DT[nt][:, n])                                      # computation of the pearson correlation\n",
    "    rs = rs.detach().numpy()\n",
    "\n",
    "    MEAN_Corre=rs.mean()\n",
    "\n",
    "    # Mean Square Error between model test and test data per neuron\n",
    "    n_steps=100\n",
    "    val_mse = np.empty((n_steps,Ntest_trials))\n",
    "    for indices in range(Ntest_trials):\n",
    "        val_mse[:,indices] = ms.test_trials_mse(m,DT[indices], ModelT[indices], n_steps)\n",
    "\n",
    "    MEAN_mse = np.mean(val_mse)\n",
    "    # Kullback Leibler Divergence\n",
    "\n",
    "    Model_Signal,_= func.concatenate_list(ModelT,0)\n",
    "    Data_Signal,_= func.concatenate_list(DT,0)\n",
    "\n",
    "    Dim_kl = int(np.floor(num_neurons/3))\n",
    "    neu_list = np.array([1,2,3])\n",
    "    kl_dim = np.ones([Dim_kl,1])*np.nan\n",
    "    for j in range(Dim_kl):\n",
    "        kl_dim[j] = kl.calc_kl_from_data(tc.tensor(Model_Signal[:,neu_list]),\n",
    "                                         tc.tensor(Data_Signal[:,neu_list]))\n",
    "        neu_list += 3\n",
    "\n",
    "    MEAN_kl = kl_dim.mean()\n",
    "\n",
    "    # Power Spectrum Error\n",
    "    MEAN_pse,pse_list = ps.power_spectrum_error(tc.tensor(Model_Signal),\n",
    "                                                tc.tensor(Data_Signal))\n",
    "    \n",
    "    #  Correlation Test trials\n",
    "    corre_list=[]\n",
    "    l_train_t=Ntraining_trials# number of training trials\n",
    "    norm_neu=[]\n",
    "    Nnorm_neu=[]\n",
    "    limit_validation=[]\n",
    "    for i_neu in range(num_neurons):\n",
    "        rs_training=[]\n",
    "        # Train correlation\n",
    "        for i_trial in range(l_train_t-1):\n",
    "            L1=NeuronPattern[\"Training_Neuron\"][i_trial][:,i_neu].shape[0]\n",
    "            for i_extra in range(i_trial+1,l_train_t):\n",
    "                L2=NeuronPattern[\"Training_Neuron\"][i_extra][:,i_neu].shape[0]\n",
    "                if L1==L2:\n",
    "                    T1=tc.from_numpy(NeuronPattern[\"Training_Neuron\"][i_trial][:,i_neu])\n",
    "                    T2=tc.from_numpy(NeuronPattern[\"Training_Neuron\"][i_extra][:,i_neu])\n",
    "                    eps=tc.randn_like(T2)*1e-5\n",
    "                    X_eps_noise=T2+eps\n",
    "                    correlation=func.pearson_r(X_eps_noise,T1)\n",
    "                    correlation=correlation.detach().numpy()\n",
    "                    rs_training.append(correlation)\n",
    "                elif L1<L2:\n",
    "                    X_rsample = signal.resample(NeuronPattern[\"Training_Neuron\"][i_extra][:,i_neu],\n",
    "                                        NeuronPattern[\"Training_Neuron\"][i_trial].shape[0])\n",
    "                    T1 = tc.from_numpy(NeuronPattern[\"Training_Neuron\"][i_trial][:,i_neu])\n",
    "                    T2 = tc.from_numpy(X_rsample)\n",
    "                    eps=tc.randn_like(T2)*1e-5\n",
    "                    X_eps_noise=T2+eps\n",
    "                    correlation=func.pearson_r(X_eps_noise,T1)\n",
    "                    correlation=correlation.detach().numpy()\n",
    "                    rs_training.append(correlation)\n",
    "                else:\n",
    "                    X_rsample = signal.resample(NeuronPattern[\"Training_Neuron\"][i_trial][:,i_neu],\n",
    "                                        NeuronPattern[\"Training_Neuron\"][i_extra].shape[0])\n",
    "                    T1 = tc.from_numpy(X_rsample)\n",
    "                    T2 = tc.from_numpy(NeuronPattern[\"Training_Neuron\"][i_extra][:,i_neu])\n",
    "                    eps=tc.randn_like(T2)*1e-5\n",
    "                    X_eps_noise=T2+eps\n",
    "                    correlation=func.pearson_r(X_eps_noise,T1)\n",
    "                    correlation=correlation.detach().numpy()\n",
    "                    rs_training.append(correlation)\n",
    "        data=np.array(rs_training)\n",
    "        corre_list.append(data)\n",
    "        norm_prob_plot = norm.ppf(np.linspace(0.01, 0.99, len(data)))  # Generate quantiles from a theoretical normal distribution\n",
    "        sorted_data = np.sort(data)\n",
    "\n",
    "        # Create a linear regression model instance\n",
    "        model = LinearRegression()\n",
    "        # Fit the model to the data\n",
    "        model.fit(norm_prob_plot.reshape(-1,1), sorted_data.reshape(-1,1))\n",
    "        # Predicting y values using the fitted model\n",
    "        y_pred = model.predict(norm_prob_plot.reshape(-1,1))\n",
    "        # Calculate R-squared\n",
    "        r_squared = r2_score(sorted_data.reshape(-1,1), y_pred)\n",
    "        # Testing normality of the correlation\n",
    "        if r_squared > 0.98:\n",
    "            norm_neu.append(i_neu)\n",
    "            # Calculate sample mean and standard deviation\n",
    "            sample_mean = np.mean(data)\n",
    "            sample_std = np.std(data,ddof=1)  # ddof=1 for sample standard deviation\n",
    "            limit_validation.append(sample_mean)#+sample_std)\n",
    "        else:\n",
    "            Nnorm_neu.append(i_neu)\n",
    "            limit_validation.append(999)\n",
    "    limit_validation=np.array(limit_validation)\n",
    "    #%Testing correlation Test Trials\n",
    "    ratio_tests=[]\n",
    "    for i in norm_neu:\n",
    "        pass_test=rs[i,:]>limit_validation[i]\n",
    "        ratio_tests.append(sum(pass_test)/Ntest_trials)\n",
    "    \n",
    "    MEAN_ceval=np.array(ratio_tests).mean()\n",
    " \n",
    "    # FIGURES\n",
    "    plt.figure()\n",
    "    for it in range(Ntest_trials):\n",
    "        plt.hist(rs[:,it],alpha=0.3)\n",
    "    plt.xlabel(\"Corr(Model vs Data)\")\n",
    "    plt.ylabel(\"neurons\")\n",
    "    plt.title(\"Distribution Test Trials\")\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Test_Distr_Corr.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    Higher=np.array([np.where(rs[:,i]>0.4)[0].shape[0]/num_neurons for i in range(Ntest_trials)])\n",
    "    Lower=np.array([np.where(rs[:,i]<0.4)[0].shape[0]/num_neurons for i in range(Ntest_trials)])\n",
    "    Trials = [i for i in range(Ntest_trials)]\n",
    "    Ratio_neurons = {\n",
    "        \">0.4\": Higher,\n",
    "        \"<0.4\": Lower,\n",
    "    }\n",
    "    width = 0.5\n",
    "    fig, ax = plt.subplots()\n",
    "    bottom = np.zeros(Ntest_trials)\n",
    "    for boolean, weight_count in Ratio_neurons.items():\n",
    "        p = ax.bar(Trials, weight_count, width, label=boolean, bottom=bottom)\n",
    "        bottom += weight_count\n",
    "    ax.set_title(\"Neuron Groups Test Trials\")\n",
    "    ax.set_ylabel(\"Ratio Neurons\")\n",
    "    ax.set_xlabel(\"Trials\")\n",
    "    lgd=ax.legend(title=\"Correlation\",bbox_to_anchor=(1.1, 1.05))\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Test_Distr_Corr_Trial.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(Ntest_trials),rs.mean(0))\n",
    "    plt.xlabel(\"Trials\")\n",
    "    plt.ylabel(\"Mean Correlation\")\n",
    "    plt.title(\"Test Trials\")\n",
    "    plt.ylim([0,1])\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Test_Correlation.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(Dim_kl),kl_dim)\n",
    "    plt.xlabel(\"Subsets of 3 neurons\")\n",
    "    plt.ylabel(\"KLx\")\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Test_KLx.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    neu=np.random.choice(num_neurons,3,replace=False)\n",
    "    ax = plt.figure(figsize=(10,10)).add_subplot(projection='3d')\n",
    "    ax.plot(Model_Signal[:,neu[0]], Model_Signal[:,neu[1]],\n",
    "            Model_Signal[:,neu[2]], 'red',linestyle='dashed',label=\"Generated\")\n",
    "    ax.plot(Data_Signal[:,neu[0]], Data_Signal[:,neu[1]], \n",
    "            Data_Signal[:,neu[2]], 'blue',linestyle='dashed',label=\"Real\")\n",
    "    lgd=ax.legend()\n",
    "    ax.set_xlabel('Neu 1',labelpad =15)\n",
    "    ax.set_ylabel('Neu 2',labelpad =15)\n",
    "    ax.set_zlabel('Neu 3',labelpad =15)\n",
    "    ax.set_title('Test')\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Test_PhasePlane.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(num_neurons),pse_list)\n",
    "    plt.xlabel(\"Neurons\")\n",
    "    plt.ylabel(\"PSE\")\n",
    "    plot_name=os.path.join(m_pathway,run+\"_Test_PSE.png\").replace('\\\\','/')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "\n",
    "    #Example Evaluation of test correlation\n",
    "    rand_neu=np.random.randint(0,num_neurons)\n",
    "    rand_test=np.random.randint(0,Ntest_trials)        \n",
    "    plt.figure()\n",
    "    plt.hist(corre_list[rand_neu],bins=20,range=(-1,1))\n",
    "    plt.axvline(rs[rand_neu,rand_test],color='r')\n",
    "    plt.axvline(limit_validation[rand_neu],color='k')\n",
    "    plt.xlabel(\"Correlation\")\n",
    "    plt.ylabel(\"Pair trials\")\n",
    "    plt.title(\"Correlation Distribution Session\")\n",
    "\n",
    "    #Ratio of test trials accepted as good trials \n",
    "    plt.figure()\n",
    "    plt.hist(np.array(ratio_tests),bins=20,range=(0,1))\n",
    "    plt.xlabel(\"Ratio of accepted test trials\")\n",
    "    plt.ylabel(\"Neurons\")\n",
    "\n",
    "    #Hyper-Parameters of the Model\n",
    "    Model_Hyper={}\n",
    "    #Identification Hidden Units\n",
    "    Model_Hyper[\"HU\"] = hyper['dim_hidden']\n",
    "    #Identification Parameter Lambda 1\n",
    "    Model_Hyper[\"L1\"] = hyper['reg_lambda1'][0]\n",
    "    #Identification Parameter Lambda 2\n",
    "    Model_Hyper[\"L2\"] = hyper['reg_lambda2'][0]\n",
    "    #Identification Parameter Lambda 2\n",
    "    Model_Hyper[\"L3\"] = hyper['reg_lambda3'][0]\n",
    "    #Identification Sequence Length\n",
    "    Model_Hyper[\"SL\"] = hyper['seq_len']\n",
    "\n",
    "    \n",
    "    #Evaluation of the Model\n",
    "    Model_Eval={}\n",
    "    #Mean Correlation of the Session vs Model\n",
    "    Model_Eval[\"Correlation\"] = MEAN_Corre\n",
    "    #Mean MSE testing in 100 sections for each trial\n",
    "    Model_Eval[\"MSE\"] = MEAN_mse\n",
    "    #Mean PSE of the whole session. \n",
    "    Model_Eval[\"PSE\"] = MEAN_pse\n",
    "    #Mean Kullback leibler divergence of the whole session\n",
    "    Model_Eval[\"KLx\"] = MEAN_kl\n",
    "    #Validation of test correlation \n",
    "    Model_Eval[\"CEval\"] =MEAN_ceval\n",
    "\n",
    "    return Model_Hyper,Model_Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff319f",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Data used in this script are the original neuronal activity. In this script we test the models trained for a specific dataset to determine optimal hyperparameters. These measurements are focused on the reconstruction of Test data (new data for the model)\n",
    "\n",
    "The input needed in this script are the directories of the original data used for training and the different models:\n",
    "\n",
    "1) data_path: pathway where the original data used for training the PLRNN is saved\n",
    "2) model_path: general directory where the different models (different hyperparameters) are saved\n",
    "3) save_path: the directory where you want to save the Dataframe generated by the script\n",
    "4) save_name: name of the file generated ('TestEvaluation_'+save_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70633049",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# Set Paths #######################################################\n",
    "# Select Path for Data (Training and Test Trials)\n",
    "data_path = 'D:\\\\_work_cestarellas\\\\Analysis\\\\PLRNN\\\\noautoencoder\\\\neuralactivity\\\\OFC\\\\CE17\\\\L6\\\\Test0\\\\datasets' \n",
    "# Select Path for Models (Folder containing the specific models to test)\n",
    "model_path = 'D:\\\\_work_cestarellas\\\\Analysis\\\\PLRNN\\\\noautoencoder\\\\results\\\\Tuning_OFC_CE17_221008'\n",
    "# Select Path for saving Data:\n",
    "save_path = 'D:\\\\_work_cestarellas\\\\Analysis\\\\PLRNN\\\\noautoencoder\\\\results\\\\Tuning_OFC_CE17_221008\\\\Evaluation_Sheets'\n",
    "# Select the name for the save file (session name):\n",
    "save_name='CE17_221008'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################ Load data ##########################################################\n",
    "\n",
    "# Load Training & Test Data\n",
    "train_n,train_i = func.load_data(data_path,'Training')\n",
    "test_n,test_i = func.load_data(data_path,'Test')\n",
    "\n",
    "Data_info={\"Training_Neuron\":train_n,\"Training_Input\":train_i,\n",
    "           \"Testing_Neuron\":test_n,\"Testing_Input\":test_i}\n",
    "\n",
    "# Load Metadata\n",
    "file=open(os.path.join(data_path,'Metadata.pkl'),'rb')\n",
    "Metadata_info=pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "######################################## Test measurements #######################################################\n",
    "\n",
    "# Computation of testing measurements for the models in your model_path\n",
    "model_list=next(os.walk(model_path))[1]\n",
    "#Initialization of evaluations lists\n",
    "Correlation=[]\n",
    "PSE=[]\n",
    "NMSE = []\n",
    "KLx=[]\n",
    "CEva=[]\n",
    "#Initialization of hyperparameter lists\n",
    "Model_name=[]\n",
    "RunNumber=[]\n",
    "hidden=[]\n",
    "lm1=[]\n",
    "lm2=[]\n",
    "lm3=[]\n",
    "sl=[]\n",
    "\n",
    "for i in tqdm(model_list,\"Testing Models: \"):\n",
    "    pathway=os.path.join(model_path,i).replace('\\\\','/')\n",
    "    runs=next(os.walk(pathway))[1] # taking only the folders with the models\n",
    "    for j in runs:\n",
    "        Hyper,Eval= Testing_eval(pathway,j,data_path,Data_info,Metadata_info)\n",
    "        # List of evaluations\n",
    "        NMSE.append(Eval[\"MSE\"])\n",
    "        Correlation.append(Eval[\"Correlation\"])\n",
    "        PSE.append(Eval[\"PSE\"])\n",
    "        KLx.append(Eval[\"KLx\"])\n",
    "        CEva.append(Eval[\"CEval\"])\n",
    "        # List of Hyper-parameters\n",
    "        # Folder's name of the model\n",
    "        Model_name.append(i)\n",
    "        # Number of the run\n",
    "        RunNumber.append(j)\n",
    "        #Identification Hidden Units\n",
    "        hidden.append(Hyper[\"HU\"])\n",
    "        #Identification Parameter Lambda 1\n",
    "        lm1.append(Hyper[\"L1\"])\n",
    "        #Identification Parameter Lambda 2\n",
    "        lm2.append(Hyper[\"L2\"])\n",
    "        #Identification Parameter Lambda 3\n",
    "        lm3.append(Hyper[\"L3\"])\n",
    "        #Identification Sequence Length\n",
    "        sl.append(Hyper[\"SL\"])\n",
    "\n",
    "\n",
    "############################################### Saving ############################################################\n",
    "\n",
    "# Saving Data as DataFrame\n",
    "TestData={\"Models\":Model_name,\"Runs\":RunNumber,\n",
    "           \"Hiddn_Units\":hidden,\"Sequence_Length\":sl,\n",
    "           \"Lambda1\":lm1,\"Lambda2\":lm2,\"Lambda3\":lm3,\n",
    "           \"Correlation\":Correlation,\"NMSE\":NMSE,\"PSE\":PSE,\n",
    "           \"KLx\":KLx,\"CEvaluation\":CEva\n",
    "          }\n",
    "Testdf=pd.DataFrame(TestData)\n",
    "\n",
    "# Check/Create Path\n",
    "if os.path.exists(save_path):\n",
    "    os.chdir(save_path)\n",
    "else:\n",
    "    os.makedirs(save_path)\n",
    "    os.chdir(save_path)\n",
    "save_file='TestEvaluation_'+save_name+'.csv'\n",
    "Testdf.to_csv(save_file,index=False)\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
